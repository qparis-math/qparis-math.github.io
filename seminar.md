---
title: Online seminar  
---
## On the interactions between Statistics and Geometry
![A.D.Alexandrov following the gradient](/images/Alexandrov.png "A.D.Alexandrov following the gradient"){: .align-right width="220px"}
This monthly online seminar invites researchers working in Geometry or Mathematical Statistics to present a paper, result or idea. The aim is to promote communication between the two fields. We encourage speakers to give synthetic and pedagogical talks of about one hour, aimed at a broad mathematical audience. Topics of interest include (but are not limited to): Statistics and Learning in metric spaces, Non-Euclidean Optimization and Sampling, Optimal Transport, Information Geometry, Manifold Learning and Functional Inequalities. 

The seminar is co-organised by  [Victor-Emmanuel Brunel](https://vebrunel.fr), [Austin Stomme](https://austinjstromme.github.io), [Alexey Kroshnin](https://www.hse.ru/en/org/persons/219293044) and [Quentin Paris](https://qparis-math.github.io).

## Next talk
>### DATE: January 16, 2026
>- **Speaker**: [Catherine Aaron](https://lmbp.uca.fr/~aaron/)
>- **Title**: Local Convex Hull for Support Estimation (and Beyond): Old, New, and Some Perspectives
>- **Abstract**: The Local Convex Hull was introduced in 2004 for home range estimation in Ecography. In this talk, we will present the original tool for estimating support and level sets. We will then demonstrate that the original tool is minimax suitable for support estimation in the "full dimensional" case and when the support is a boundaryless manifold. Regarding level sets, a slight modification of the original method enables consistent estimation in a fairly general setting. Local Convex Hull is closely related to the convexity defect function and can thus also be applied to reach estimation. We will also present perspectives on this tool. The introduction of local parameters, unfortunately, risks inducing a loss of robustness. We will also discuss computational aspects.
>- **Time**: 3pm (CET)
>- **Zoom**: [Link](https://zoom.us/j/96365475425?pwd=BZMVAaxGbSaYgClFxENbKtOByahaHe.1) (ID:963 6547 5425, PW:854931)

## Upcoming scheduled talks
TBA

## Past talks
### June 27, 2025
- **Speaker**: [Gil Kur](https://sites.google.com/view/gilkur/)
- **Title**: On the Role of Gaussian Covariates in Minimum Norm Interpolation
- **Abstract**: In the literature on benign overfitting in linear models, also referred to as minimum norm interpolation, it is typically assumed that the covariates follow a Gaussian distribution. Existing proofs heavily rely on the Gaussian Minimax Theorem (GMT), making them inapplicable to other distributions in the linear setting. In our work, we are the first to establish matching rates for sub-Gaussian covariates in $\ell_p$-linear regression through a novel approach inspired by modern functional analysis. In this talk, we provide an overview of this proof and explore the role of Gaussian covariates in benign overfitting from a purely geometric perspective.
- **Recording**: NA
  
### April 4, 2025
- **Speaker**: [Nicolas Boumal](https://www.nicolasboumal.net)
- **Title**: Saddle point avoidance in optimization with deterministic algorithms: refined proof techniques
- **Abstract**: It is an unfortunate fact that reasonable optimization algorithms may converge to saddle points (for our purpose: critical points which are not even local minimizers). That said, this is exceedingly rare. Typically, if the initialization is generic (e.g., if it is random), then we do not see such bad behavior. This was first formalized for gradient descent with constant step size by Lee et al. (2016). Since then, several extensions were proposed, but all of them are restricted to simple algorithms (essentially iterating a nice map). The proofs (which rely on the Center Stable Manifold Theorem) do not extend to seemingly simple variations such as gradient descent with a line-search method. I will present ongoing work with Andreea Musat where we refine existing proof techniques to extend them to a first line-search method. These results also apply for Riemannian optimization, with some friction.
- **Slides**: [Link](https://drive.google.com/file/d/1YaEZQFG9Qv23hwuESMhs8HraC_AWcRBY/view?usp=sharing)
- **Recording**: NA

### February 28, 2025
- **Speaker**: [Cyril Letrouit](https://www.imo.universite-paris-saclay.fr/~cyril.letrouit/) (CNRS, Laboratoire de Mathématiques d'Orsay)
- **Title**: Quantitative stability of optimal transport maps
- **Abstract**: Optimal transport consists in sending a given source probability measure to a given target probability measure, in a way which is optimal with respect to some cost. On bounded subsets of R^d, if the cost is given by the squared Euclidean distance and the source measure is absolutely continuous, a unique optimal transport map exists. The question we will discuss is the following: how does this optimal transport map change if we perturb the target measure? For instance, if instead of the target measure we only have access to samples of it, how much does the optimal transport map change? This question, motivated by numerical and statistical aspects of optimal transport, has started to receive partial answers only recently, under quite restrictive assumptions on the source measure. We will review these answers and show how to handle much more general cases. This is a joint work with Quentin Mérigot.
- **Recording**: [Link](https://drive.google.com/file/d/10R8zCZaqubP2RrT3V_1wm0pwOmawnbvJ/view?usp=sharing)


### December 13, 2024
- **Speaker**: [Flavien Léger](https://flavienleger.github.io/) (INRIA, Université Paris Dauphine)
- **Title**: A lecture on the interface between information geometry, optimization and optimal transport
- **Abstract**: I will first introduce some basic concepts in information geometry, explained through the lens of the little-known framework of para-Kähler geometry. I will then present recent applications of this framework to first-order optimization methods à la mirror descent, geometric formulas for asymptotics of entropic transport problems, as well as apriori estimates for optimal transport maps. 
- **Recording**: [Link](https://drive.google.com/file/d/1hi4XJTSUsJWGECrc0sqQ_r9EOvurXoyk/view?usp=sharing)

### November 8, 2024
- **Speaker**: [Dario Trevisan](https://cvgmt.sns.it/person/1259/) (Universita degli Studi di Pisa)
- **Title**: Asymptotics for Random Quadratic Transportation Costs
- **Abstract**: We establish the validity of asymptotic limits for the general transportation problem between random i.i.d. points and their common distribution, with respect to the squared Euclidean distance cost, in any dimension larger than three. Previous results were essentially limited to the two (or one) dimensional case, or to distributions whose absolutely continuous part is uniform.
The proof relies upon recent advances in the stability theory of optimal transportation, combined with functional analytic techniques and some ideas from quantitative stochastic homogenization. The key tool we develop is a quantitative upper bound for the usual quadratic optimal transportation problem in terms of its boundary variant, where points can be freely transported along the boundary. The methods we use are applicable to more general random measures, including occupation measure of Brownian paths, and may open the door to further progress on challenging problems at the interface of analysis, probability, and discrete mathematics.
Based on joint work with M. Huesmann and M. Goldman (arXiv:2409.08612)
- **See Dario's blog post**: [Link](http://people.dm.unipi.it/trevisan/blog/posts/QParis-2024/) 
- **Recording**: [Link](https://drive.google.com/file/d/1Boi7fEndqtKrKcqbnT8sgXWsmVEQVC-d/view?usp=sharing)

### May 31, 2024
- **Speaker**: [Giuseppe Savaré](https://dec.unibocconi.eu/people/giuseppe-savare) (Bocconi University)
- **Title**: The construction of Dirichlet forms and Sobolev spaces on the Wasserstein space
- **Abstract**: The talk will concern the construction of a class of Dirichlet forms and corresponding Sobolev spaces induced by a finite reference measure on the L^2-Kantorovich-Wasserstein space of probability measures on R^d (or a Riemannian manifold). Such forms can be characterised by at least two different approaches. The first is based on the closure of the canonical energy form on smooth cylinder functions that arise from Otto calculus. The second relies on the Cheeger energy, which is defined in terms of the underlying Wasserstein metric by integrating the squared asymptotic Lipschitz constant of Lipschitz functions. The equivalence of these two approaches is a consequence of general approximation results for Sobolev spaces in metric-measure spaces. (In collaboration with Massimo Fornasier and Giacomo Sodini)
- **Recording**: [Link](https://drive.google.com/file/d/15kfUJ7Tyu2LaQEhbt_nbUkHx3zJ1WnOU/view?usp=sharing)

### May 3, 2024
- **Speaker**: [Govind Menon](https://www.dam.brown.edu/people/menon/) (Brown University)
- **Title**: The Riemannian Langevin equation: models and sampling schemes
- **Abstract**: The rigorous foundations of Brownian motion on Riemannian manifolds was developed in the 1970s. However, our understanding of this problem, in particular the interplay between the underlying metric and the Brownian motion has been considerably enriched by recent applications. In several recent works, we have used this theory to design Riemannian Langevin equations, all of which correspond to stochastic gradient descent of entropy. We will describe two such examples in this talk:
  - (a) A low-regularity construction of Brownian motion  (with Dominik Inauen)
  -  (b) Gibbs sampling with Riemannian Langevin Monte Carlo schemes (with Jianfeng Lu, Tianmin Yu, Xiangxiong Zhang and Shixin Zheng)
- **Recording**: [Link](https://drive.google.com/file/d/18_E0jpFZbUuby-BOFrWqHvUi_3WHjfEt/view?usp=sharing)
   
### April 5, 2024
- **Speaker**: [Sinho Chewi](https://chewisinho.github.io) (IAS, Princeton)
- **Title**: Variational inference via Wasserstein gradient flows
- **Abstract**: Variational inference (VI), which seeks to approximate the Bayesian posterior by a more tractable distribution within a variational family, has been widely advocated as a scalable alternative to MCMC. However, obtaining non-asymptotic convergence guarantees has been a longstanding challenge. In this talk, I will argue that viewing this problem as optimization over the Wasserstein space of probability measures equipped with the optimal transport metric leads to the design of principled algorithms which exhibit strong practical performance and are backed by rigorous theory. In particular, we address Gaussian VI, as well as (non-parametric) mean-field VI.
- **Recording**: [Link](https://drive.google.com/file/d/1N8sSWSL8ecZS4z-gMwrEU4XEn3PBB_fc/view?usp=sharing)

### March 8, 2024
- **Speaker**: [Lénaïc Chizat](https://lchizat.github.io/) (EPFL, Institute of Mathematics)
- **Title**: Doubly Regularized Entropic Wasserstein Barycenters
- **Abstract**: Wasserstein barycenters are natural objects to summarize a family of probability distributions, but they suffer from the curse of dimensionality, both statistically and computationally. In this talk, I will propose a new look at the entropic regularization of Wasserstein barycenters. I will show that, via a *double entropic regularization* of the problem, one obtains a notion of barycenter with none of these drawbacks. In addition, and perhaps counter-intuitively, with well-chosen regularization strengths, this double regularization approximates the true Wasserstein barycenter better than with a single regularization. In this talk, the barycenter problem serves as a common thread to present recent results in the theory of entropic optimal transport from a statistical, approximation and computational viewpoint, which are relevant in more general contexts. References: [Ref 1](https://arxiv.org/abs/2303.11844), [Ref 2](https://arxiv.org/abs/2307.13370)
- **Recording**: [Link](https://drive.google.com/file/d/1XK1NTM7hi5JSVLCkdun2HZGYyafiLnzk/view?usp=sharing)
  
### February 9, 2024
- **Speaker**: [Santosh Vempala](https://faculty.cc.gatech.edu/~vempala/) (Georgia Tech)
- **Title**: High-dimensional Sampling: From Euclid to Riemann 
- **Abstract**: Sampling high-dimensional densities is a basic algorithmic problem that has led to mathematically interesting tools and techniques. Many sampling algorithms can be viewed as discretizations of suitable continuous stochastic processes, raising the questions: Which stochastic process to use? And how to discretize it? In this talk, we discuss the use of Riemannian metrics to guide sampling algorithms and how, perhaps surprisingly, they can lead to improvements even for Euclidean sampling.  We will focus on two methods -- Riemannian Langevin and Riemannian Hamiltonian Monte Carlo –- and highlight some open questions.
- **Recording**: [Link](https://drive.google.com/file/d/1L8x2IG4IQ1KEf5_fha15uFFXlK2X9CBa/view?usp=sharing)

### January 19, 2024
- **Speaker**: [Eugene Stepanov](https://cvgmt.sns.it/person/26/) (PDMI RAS, Universita di Pisa, HSE University)
- **Title**: Eigenvalues and eigenvectors of squared distance matrices and geometry of metric measure spaces
- **Abstract**: We will discuss what the spectral data of matrices of squared distances  between points from very large subsets (covering densely the space in the limit) of a metric measure space say about the geometry of the latter. In particular, we will discuss how the metric measure space can be reconstructed from such data.
- **Recording**: [Link](https://drive.google.com/file/d/1M8KsYc_VwY4e_3GMD4WHZRohS2_9x9KB/view?usp=sharing)
